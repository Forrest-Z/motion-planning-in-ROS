<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ROS Motion Planning from Scratch: Motion Planning in ROS from Scratch</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">ROS Motion Planning from Scratch
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('index.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Motion Planning in ROS from Scratch </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Overview</h2>
<p>This project is in progress.</p>
<p>Brief Package Descriptions:</p><ul>
<li><code>roadmap</code>: A package with tools to generate various types of graph structured Road Maps. Currently, it supports PRMs and Grids</li>
</ul>
<p>Planned additions:</p><ul>
<li>Global Planning using Theta*, D* Lite, Potential Fields</li>
<li>Local Planning with DWA and MPC</li>
</ul>
<p>See the <a href="https://rencheckyoself.github.io/motion-planning-in-ROS/">Full API</a> for more info.</p>
<h2>How to use</h2>
<h3>Probabilistic Road Maps</h3>
<p>To generate a PRM launch <code>roadmap view_prm.launch</code>. This will create a new PRM and visualize it in Rviz.</p>
<ul>
<li>Change the parameters in <code>roadmap/config/map_params.yaml</code> to customize the components of the map.</li>
</ul>
<p>The following image was taken using a cell size of 0.2m with a buffer radius of 0.15m. The graph consists of 500 nodes trying to connect to the 10 nearest neighbors.</p>
<div class="image">
<img src="roadmap/documentation/prm_example.png" width="500"/>
</div>
<h3>Grids</h3>
<p>To generate a grid, launch <code>roadmap view_grid.launch</code>. This will create a new grid and visualize it in Rviz.</p>
<ul>
<li>Change the parameters in <code>roadmap/config/map_params.yaml</code> to customize the components of the map.</li>
</ul>
<p>The following image was taken using a cell size of 0.2m with a buffer radius of 0.15m. The grid has a 5 times finer resolution than the provided map, with black cells as the actual obstacle, gray cells representing cells inside the buffer zone, and white representing the free space.</p>
<div class="image">
<img src="roadmap/documentation/grid_example.png" width="500"/>
</div>
<h3>Heuristic Search on a Known Map (A* and Theta*)</h3>
<p>To view the algorithm in action, launch <code>global_search plan_prm.launch</code>. This will create a PRM graph, apply A* and Theta* search to it, and visualize the results it in Rviz.</p>
<ul>
<li>Change the parameters in <code>roadmap/config/map_params.yaml</code> to customize the components of the map.</li>
<li>Change the parameters in <code>global_search/config/search_params.yaml</code> to change the start and goal locations.</li>
</ul>
<p>The following image was taken using a cell size of 0.2m with a buffer radius of 0.15m. The graph consists of 500 nodes trying to connect to the 10 nearest neighbors. The green node is the start and the red node is the goal. The black line is the path determined by A* and the orange line is the path determined by Theta*.</p>
<div class="image">
<img src="global_search/documentation/Thetastar_v_Astar.png" width="500"/>
</div>
<h3>Iterative Search on an Unknown Map (LPA* and D* Lite)</h3>
<p>To view the LPA* algorithm, launch <code>global_search lpastar_grid.launch</code>. This will create 2 grids, one using the stored obstacle data and one only accounting for the map boundary. LPA* is provided the empty grid and will plan an initial path between the start and goal locations. The known grid will be used to simulate a camera or some other sensor detecting a change in the environment, which will trigger LPA* to replan given the new information.</p>
<ul>
<li>Change the parameters in <code>roadmap/config/map_params.yaml</code> to customize the components of the map.</li>
<li>Change the parameters in <code>global_search/config/search_params.yaml</code> to change the start and goal locations.</li>
</ul>
<p>The following gif was taken using a cell size of 0.2m with a buffer radius of 0.15m and a grid resolution of 1. The green node is the start and the red node is the goal, with the black line showing the final path determined by LPA* for the current map data. The faded area of the map is assumed by the search to be completely free and occupancy data is filled in one row at a time from the bottom up. Cells marked with a light blue square indicate that it was updated during the most recent search.</p>
<div class="image">
<img src="global_search/documentation/lpastar.gif" width="500"/>
</div>
<p>To view the D* Lite algorithm, launch <code>global_search dstarlite_grid.launch</code>. This will create 2 grids, one using the stored obstacle data and one only accounting for the map boundary. D* Lite is provided the empty grid and will plan an initial path between the start and goal locations. The known grid will be used to simulate a sensor mounted to the robot to detect a change in the environment within a given radius around the robot. This will trigger D* Lite to replan given the new information.</p>
<ul>
<li>Change the parameters in <code>roadmap/config/map_params.yaml</code> to customize the components of the map.</li>
<li>Change the parameters in <code>global_search/config/search_params.yaml</code> to change the start and goal locations and the sensor range.</li>
</ul>
<p>The following gif was taken using a cell size of 0.2m with a buffer radius of 0.15m, a grid resolution of 1, and a simulated sensor range of 0.6m. The green node is the start and the red node is the goal and the robot is the blue cube. The black line represents the path the robot has taken and the orange line is the path determined by D* Lite for the current map data. The faded area of the map is assumed by the search to be completely free and occupancy data is filled as the simulated sensor is able to detect the cell. Cells marked with a light blue square indicate that it was updated during the most recent search.</p>
<div class="image">
<img src="global_search/documentation/dstarlitev2.gif" width="500"/>
</div>
<h3>Potential Fields</h3>
<p>To view the algorithm in action, launch <code>global_search plan_potential_fields.launch</code>. This will use the existing map data to plan a path from start to goal using the standard potential field algorithm. This implementation does not currently provide a means of escaping local minima and assumes a fully known map.</p>
<ul>
<li>Change the parameters in <code>roadmap/config/map_params.yaml</code> to customize the components of the map.</li>
<li>Change the parameters in <code>global_search/config/search_params.yaml</code> to change the start and goal locations and potential field parameters.</li>
</ul>
<p>The following gif was taken using a cell size of 0.2 with the following potential field parameters: </p><div class="fragment"><div class="line">att_weight: 0.6 # weighting factor the attactive component</div><div class="line">dgstar: 3 # piecewise threshold for attractive gradient</div><div class="line">rep_weight: 0.1 # weighting factor the repulsive component</div><div class="line">Qstar: 0.4 # obstacle range of influence</div><div class="line">epsilon: 0.05 # termination threshold</div><div class="line">zeta: 0.01 # step size</div></div><!-- fragment --><p> The green node is the start and the red node is the goal and the orange line is the path determined by the potential field algorithm.</p>
<div class="image">
<img src="global_search/documentation/potfield.gif" width="500"/>
</div>
<h3>MPPI</h3>
<p>To view the algorithm in action, launch <code>mppi_control turtlebot_mppi.launch</code>. After launching, call the <code>/start</code> service from the terminal to begin the waypoint following. This will apply the mppi control algorithm to calculate a control sequence to drive the robot to a series of waypoints. This package depends on a couple of packages located in my other <code>ros_navigation_from_scratch</code> repo. Use the included .rosinstall file to ensure you get the correct packages.</p>
<ul>
<li>Change parameters in the mppi_control/config/control_param.yaml to tune the controller</li>
</ul>
<p>This example is using MPPI control to select wheel velocities for a differential drive robot to drive through consecutive waypoints. Each waypoint is an (x,y,heading) tuple. The parameters used are shown in the configuration file. Due to the random sampling each run is slightly different, but exhibit the same general behavior.</p>
<div class="image">
<img src="mppi_control/documentation/ROS_mppi_waypoints.gif" width="500"/>
</div>
<p>Also included in <code>mppi_control/testing_files</code> is a python-only script to perform the same algorithm. To use this script, execute the <code>mppi.py</code> file. It is currently configured to have a unicycle model robot follow waypoints. Below are some results for various robots and tasks:</p>
<p>The first plot is using the unicycle kinematic model to solve the parallel parking problem. The output of the control algorithm is linear and angular velocities. See the python script for all of the parameters.</p>
<div class="image">
<img src="mppi_control/testing_files/unicycle.gif" width="500"/>
</div>
<p>The plot below is using the differential drive kinematic model to solve the parallel parking problem. The output of the control algorithm is right and left wheel velocities. See the python script for all of the parameters.</p>
<div class="image">
<img src="mppi_control/testing_files/diff_drive.gif" width="500"/>
</div>
<p>The third plot is using the unicycle kinematic model to follow a series of waypoint. The output of the control algorithm is linear and angular velocities. See the python script for all of the parameters.</p>
<div class="image">
<img src="mppi_control/testing_files/waypoints-unicycle.gif" width="500"/>
</div>
<h2>Background</h2>
<h3>Probabilistic Road Map</h3>
<p>A PRM is a means to efficiently constructing a system of valid pathways through an environment as it has the advantage to plan in high dimensional configuration spaces. The assembly starts by randomly sampling states and only keeping them if they are a valid. In this implementation, a valid node is an x,y position that is not within the bounds of an obstacle or its buffer zone. After N number of valid nodes have been sampled each node is connected to it's k-nearest neighbors along valid straight line paths. In this implementation a path or edge is considered valid if it does not intersect an obstacle or it's buffer zone.</p>
<p>The challenging part of implementing a PRM is identifying how to determine if a node/edge is valid. This implementation currently only supports convex obstacles and expects that the vertices are provided in counterclockwise order. The collision detection is as follows:</p><ul>
<li>To determine if a sampled node is inside of an obstacle, test if the state is on the same side of all of the line segments.</li>
<li>To determine if a sampled node is inside the buffer zone, calculate that shortest distance to each line segment and compare it to the desired buffer distance.</li>
<li>To determine if an edge between two nodes is</li>
</ul>
<h2>References and Resources</h2>
<ul>
<li>LaValle, Steven M. Planning algorithms. Cambridge university press, 2006.</li>
<li>Choset, Howie M., et al. Principles of robot motion: theory, algorithms, and implementation. MIT press, 2005.</li>
<li>Latombe, Lydia E. Kavraki Jean-Claude. ”Probabilistic Roadmaps for Robot Path Planning.” Prati- cal motion planning in robotics: current aproaches and future challenges (1998): 33-53.</li>
<li>Daniel, Kenny, et al. ”Theta*: Any-angle path planning on grids.” Journal of Artificial In- telligence Research 39 (2010): 533-579.</li>
<li>Koenig, Sven, and Maxim Likhachev. ”Fast replanning for navigation in unknown terrain.” IEEE Transactions on Robotics 21.3 (2005): 354-363.</li>
<li>Williams, Grady, Andrew Aldrich, and Evangelos Theodorou. "Model predictive path integral control using covariance variable importance sampling." arXiv preprint arXiv:1509.01149 (2015).</li>
<li>Abraham, Ian, et al. "Model-Based Generalization Under Parameter Uncertainty Using Path Integral Control." IEEE Robotics and Automation Letters 5.2 (2020): 2864-2871. </li>
</ul>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
